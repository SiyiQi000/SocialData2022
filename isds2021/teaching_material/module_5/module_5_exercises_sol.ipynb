{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Videos and Exercises for Session 5: Strings, Requests and APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Assignment 0 we covered a lot about basic Python, and in the first sessions, we talked a bit about how to deal with different data types in pandas - strings among other. However, we only scratched the surface on string-related topics and in this session, we will be taking a deeper looking at. \n",
    "\n",
    "We will start out with a recap of some basic string operations and build on that to make a query for fetching data. After having finished this combined teaching module and exercise set, you will know the basics of collecting from the web. This notebook is structured as follows:\n",
    "1. String Operations (recap and warm-up):\n",
    "    - Common string operations\n",
    "    - More string operations\n",
    "    - Warm-up exercises\n",
    "2. Saving as Text File\n",
    "3. Python Containers and Dictionaries\n",
    "4. Python and the Web\n",
    "    - Application Programming Interface (API)\n",
    "    - The Punk API\n",
    "    - The API for Statistics Denmark\n",
    "5. Bonus Exercises $-$ Traffic Data in Copenhagen\n",
    "\n",
    "*Alternative sources*: If you get lost, you might find [this page](https://pythonprogramming.net/string-concatenation-formatting-intermediate-python-tutorial/) on pythonprogramming.net useful. [This page](https://www.python-course.eu/python3_sequential_data_types.php) also gives an introduction to the basics of strings and their related data types. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: String Operations\n",
    "\n",
    "As we have already learned, strings are sequential containers of characters. In Python we use two kinds of characters:\n",
    "-  American Standard Code (`ascii`)\n",
    "    - Characters from English alphabet, numbers, symbols for writing \n",
    "    - 8 bit information\n",
    "\n",
    "- Unicode (`UTF`)  \n",
    "    - Characters from European and Asian language and much more\n",
    "    - 16 bit information\n",
    "    - Available in URLs recently, e.g. [møn.dk](https://møn.dk)        \n",
    "    \n",
    "    \n",
    "Note that while unicode is a little more heavy and costs more space, it is way more flexible and we get less errors when importing data from non-ascii languages. This reason is also why Python adopted unicode as standard since the release of Python 3.\n",
    "\n",
    "To see that strings are sequential containers, see the example below where we slice them like a list or dataframe/array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 = 'police'\n",
    "str1[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common string operations \n",
    "\n",
    "Strings have multiple operations and functions associated. In this exercise, we investigate a few of these. We also explore the sequence form of a string and how it can be sliced and accessed via indices. In the following, we provide a small tour of some of the most important technical features of strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that can be helpful when working with strings is that you can alter the sentence-case of strings by using the string methods `upper`, `lower`, `capitalize`. Consider the example below and try switching `upper()` with one of the other operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the `replace` method to substitute parts of the strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1.replace('po', 'ma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check whether a substring is within a given string and much more. The general syntax is \n",
    "```python \n",
    "T in S\n",
    "``` \n",
    "which checks whether a string `S` contains the substring `T`. We should always get a boolean True/False outcome in return. See two applications below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ice' in str1, 'mice' in str1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another procedure is to add strings together like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str2 = 'officer'\n",
    "str1 + ' ' + str2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be helpful when we want to combine strings of words into sentences or long text files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More string operations \n",
    "In addition to the techniques above, strings are equipped with an array of _methods_, for solving more complex tasks. For example:\n",
    "- `str.join(list)` will insert a string in between each element of a list\n",
    "- `str.strip()` removes spaces in the beginning and end of a word \n",
    "- the f-string fills in specified blanks in a string\n",
    "- `str1.split(str2)` splits `str1` into a list through all places where `str2` shows up\n",
    "\n",
    "Below we illustrate the use of each function:\n",
    "\n",
    "```python\n",
    ">>> \" \".join(['Hello', 'World!']) \n",
    "'Hello World!'\n",
    "\n",
    ">>> ' Hello World!   '.strip() \n",
    "'Hello World!'\n",
    "\n",
    ">>> w = 'World'\n",
    ">>> f'Hello {w}' \n",
    "'Hello World!'\n",
    "\n",
    ">>> 'a,b,c'.split(',') \n",
    "['a','b','c']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-up String Exercises\n",
    "\n",
    "In the first couple of exercises, you should use the examples above to solve a set of tasks.\n",
    "\n",
    "> **Ex. 5.1.1**: Let `s1='Chameleon'` and `s2='ham'`. Check whether the string `s2` is a substring of `s1`. Is `'hello'` a substring `'goodbye'`?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-993caea0991d84c4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "s1, s2 = 'chameleon', 'ham'\n",
    "#s1, s2 = 'hello', 'goodbye'\n",
    "\n",
    "if s1 in s2:\n",
    "    print(s1.capitalize() + ' is indeed part of ' + s2)\n",
    "else:\n",
    "    print(s1.capitalize() + ' is not part of ' + s2)\n",
    "    \n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.1.2**: From the string `s1` select the last four characters. What is the index of the character `a` in `s1`?\n",
    "\n",
    "> *Hint*: We can selecti a substring by slicing it with the `[]` notation, from the start to end where start is included and end is excluded. Recall that Python has zero-based indexing, see explanation [here](https://softwareengineering.stackexchange.com/questions/110804/why-are-zero-based-arrays-the-norm).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ace323777447ea4a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "s1, s2 = 'chameleon', 'ham'\n",
    "\n",
    "print(s1[-4:])\n",
    "print(s1.index('a'))\n",
    "    \n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.1.3:** Use the `join()` and `strip()` functions to retrieve the sentence `The quick brown fox jumps over the lazy dog` from the list  `list_of_words` in the code cell below. \n",
    "\n",
    "> *Hint:* If you want to challenge yourself, see if you can do this in one line with a list comprehension!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_words = ['       The        ', '   quick   ', '     brown      ',\n",
    "                 ' fox          ', '          jumps     ', '   over ',\n",
    "                 '          the   ', '  lazy     ', '          dog     ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c07cac510187b734",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "print(' '.join([list_of_words[i].strip() for i in range(len(list_of_words))]))\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.1.4:** Let `l1 = ['r ', 'Is', '>', ' < ', 'g ', '?']`. Create from `l1` the sentence \"Is r > g?\" using your knowledge about string formatting. Make sure there is only one space in between worlds.\n",
    ">\n",
    "> _Hint:_ You should be able to combine the above information to solve this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9dafd1e8c334c195",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "l1 = ['r ', 'Is', '>', ' < ', 'g ', '?']\n",
    "print(l1[1]+' '+l1[0]+l1[2]+' '+l1[4][0]+l1[5])\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part 2: Saving as Text File\n",
    "In this course, we have already seen that we can output tabular data as CSV files. These files are essentially text files with specific structure that allows a computer to identify rows and columns. \n",
    "\n",
    "In the example below, we will learn how to actually save a string directly as a text file. Note how we make a line break using `\\n` - this is a string escape sequence, read more [here](https://docs.python.org/3/reference/lexical_analysis.html#literals)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "my_str = 'This is important...'\n",
    "my_str2 = 'Written in Python!'\n",
    "escape_seq = '\\n'\n",
    "\n",
    "with open('my_file.txt', 'w') as f:\n",
    "    f.write(my_str+escape_seq+my_str2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The code below opens the text file and prints the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with open('my_file.txt', 'r') as f:    \n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.2.1:** Create a .txt file called `to_do_list.txt` with a to-do list by looping over the list `to_do = ['1. Hit the gym', '2. Pay bills', '3. Meet George', '4. Buy eggs', '5. Read a book']` and writing each element on a seperate line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_do = ['1. Hit the gym', '2. Pay bills', '3. Meet George', '4. Buy eggs', '5. Read a book']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e5321552aeca36ad",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "with open('to_do_list.txt', 'w') as f:\n",
    "    for i in range(len(to_do)):\n",
    "        f.write(to_do[i]+'\\n')\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Does the file look like you expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with open('to_do_list.txt', 'r') as f:    \n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Python Containers and Dictionaries\n",
    "\n",
    "Dictionaries (or simply `dict`) are a central building block of python. Python dicts are constructed from pairs of keys and values, making them extremely versatile for data storage. Like list, they can contain deep nested structures, e.g. a dict of dicts of lists. You can learn more about this in the following video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('iRju871QsNc', width=640, height=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try running the code below, where the keys are strings (names from recent Danish prime ministers) and values are also strings (e.g. political affiliation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "my_dict1 = {'Anders': \"Venstre\",\n",
    "            'Helle': \"Socialdemokratiet\",\n",
    "            'Lars': \"Venstre\",\n",
    "            'Mette': \"Socialdemokratiet\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now,  print the party of some (former) Danish prime minister by replacing `'Name'` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_dict1['NAME'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.3.1**: Create an empty dictionary `words` using the `dict()`function. Then add each of the words in `['animal', 'coffee', 'python', 'unit', 'knowledge', 'tread', 'arise']` as a key, with the value being a boolean indicator for whether the word begins with a vowel. The results should look like `{'bacon': False, 'asynchronous': True ...}`.\n",
    "\n",
    "> _Hint:_ One approach is to first construct a function that asseses whether a given word begins with a vowel or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-81e592c15ce8ee6d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "W =  ['animal', 'coffee', 'python', 'unit', 'knowledge', 'tread', 'arise']\n",
    "vowels = ['a', 'e', 'i', 'o', 'u', 'y']\n",
    "words = {}\n",
    "\n",
    "for i in range(len(W)):\n",
    "    words[W[i]] = W[i][0] in vowels\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.3.2:** Loop through the dictionary `words`. In each iteration, you should print a proper sentence stating if the current word begins with a vowel or not. \n",
    "\n",
    "> _Hint:_ You can loop through both keys and values simultaneously with the `.items()` method. [This](https://www.tutorialspoint.com/python/python_dictionary.htm) and [this](https://stackoverflow.com/questions/3294889/iterating-over-dictionaries-using-for-loops) might help you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-333a6d7713f78839",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "for key, value in words.items():\n",
    "    if value == True:\n",
    "        print(key.capitalize() + ' begins with a vowel!')\n",
    "    else:\n",
    "        print(key.capitalize() + ' does not begin with a vowel...')\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the video, we also introduced the popular JSON file format which can store lists and dictionaries and hold the three fundamental data types `float`, `int` and `str`. The code example below uses the JSON module to save our dictionary. We use a trick by first converting the JSON file to a string. This can be done with the function `dumps` in the module `json`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('my_file.JSON', 'w') as f:\n",
    "    my_json_str = json.dumps(words) # convert dictonary to string with JSON formatting\n",
    "    f.write(my_json_str) # write the string to file\n",
    "\n",
    "with open('my_file.JSON', 'r') as f:\n",
    "    print(f.read()) # read the string from file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.3.3:** Install `https://pypi.org/project/Random-Word/` (documentation [here](https://pypi.org/project/Random-Word/)) by typing `pip install random-word` in your *Anaconda prompt*, and then run the line below. Doing so, you should get a list of 500 randomly drawn words. Now, create a dictionary that contains the length of each of these words (i.e. like `{'livering': 8, 'reiterate': 9,...}`) and save it as a json-file called `r_words.JSON`. Then read it and make sure that you got everything right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random_word import RandomWords\n",
    "r = RandomWords()\n",
    "r_words = r.get_random_words(limit=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c4a946f4ae699bfc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "word_dict = {}\n",
    "for i in range(len(r_words)):\n",
    "    word_dict[r_words[i]] = len(r_words[i])\n",
    "    \n",
    "with open('r_words.JSON', 'w') as f:\n",
    "    my_json_str = json.dumps(word_dict) # convert dictonary to string with JSON formatting\n",
    "    f.write(my_json_str) # write the string to file\n",
    "\n",
    "with open('r_words.JSON', 'r') as f:\n",
    "    print(f.read()) # read the string from file\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Part 4: Python and the Web\n",
    "\n",
    "The internet is a massive source for collecting data. Watch the video below to get an overview of the most fundamental protocols and how we work with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo('cOfo3fRt05w', width=640, height=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Application Programming Interface (API)\n",
    "\n",
    "APIs are protocols that allow us to request information and/or services from the provider of the API. In this course, we are mainly interested in APIs that provide data as a response to our requests. Watch the video below to get a sense of what APIs exist and how they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo('llFtQKWBApU', width=640, height=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Queries\n",
    "We will now move on to understanding how we can interact with a web API in Python. First we will see how to build a query, which is simply a web address. By typing in a specific web address, the web server will receive information from us.\n",
    "\n",
    "In the example below we build a URL that allows us to check out which repositories ISDSUCPH has publicly available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "server_url = 'https://api.github.com'\n",
    "endpoint_path = '/users/isdsucph/repos'\n",
    "\n",
    "url = server_url + endpoint_path\n",
    "\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sending the Query\n",
    "Python has a smart module, named `requests` (get it with `pip install requests`), that allows us to interact with the web. When we request a URL, we get a response in return. Among other things, it allows us to inspect the HTML code. In the example below, we query the URL for the GitHub API we made above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import requests # import the module requests\n",
    "response = requests.get(url) # submit query with `get` and save response as object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we examine the response, we can see the HTML string is pretty long, so we limit the initial output to the first 1,000 characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(len(response.text),'\\n') # print length of HTML\n",
    "print(response.text[:1000]) # print first 1,000 characters of HTML "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking closely, we notice how the structure resembles lists and dictionaries (note that depending on the context, these responses can be VERY difficult to make sense of from simply printing - just as in this case). Therefore, we try to convert it by assuming that it is structured as JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response_json = response.json() # convert response to a list of dicts\n",
    "response_json[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is easier to see that this is in fact a dictionary. Looking more closely at the output, we can also see that the depth of the dictionary is not trivial - it contains dictionaries of dictionaries.\n",
    "\n",
    "In order to make the output more digestable, use the so-called *pretty printer* that, among other things, arranges the output alphabetically and makes the depth of the output more clearly visible. Take a look and compare with what we had above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint #Data pretty printer\n",
    "pprint.pprint(response.json()) #Everything is aranged alphabetically and in appropriate levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Punk API\n",
    "\n",
    "Now, it is time for you to request information through an API. First, we will work with the punk API that contains information on a large selection of beers. In this part of exercises, you are going to navigate in the information on these beers. Second, you are going to extract information from the DST (Statistics Denmark) API.\n",
    "\n",
    "The [punk API](https://punkapi.com/) serves information about _beers_. It is a well made and well documented API which makes it great for learning about APIs. \n",
    "\n",
    "> **Ex. 5.4.1:** Read the documentation on the Punk API available [here](https://punkapi.com/documentation/v2). What is the server url (i.e. root endpoint) of the Punk API? Does it require authentication? Then use the Punk API to make a request for the first 80 beers brewed after April, 2004 with an ABV of at least 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d6af29d54fd7ded7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "# Server URL is 'https://api.punkapi.com/v2/'\n",
    "# No authentication required \n",
    "\n",
    "import requests\n",
    "response = requests.get('https://api.punkapi.com/v2/beers?per_page=80&brewed_after=4-2004&abv_gt=5')\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.4.2:** What object type is the API's JSON response? What about the individual items in the container? Convert the response object to a suitable format and answer the following questions:\n",
    "> 1) How many beers are there in the JSON object?\n",
    ">\n",
    "> 2) Print the response with and without the pretty printer. See any difference?\n",
    ">\n",
    "> 3) Now print only the names of the beers in the JSON object using lower case characters.\n",
    ">\n",
    "> 4) Select the beer called Rabiator from the JSON object. What is the suggested food pairing?\n",
    ">\n",
    "> 5) Select the beer called Hello My Name Is Beastie from the JSON object. Which malt ingredients does the Hello My Name Is Beastie contain?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1c6a5eef920b6c70",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "# format is json (see documentation)\n",
    "beers = response.json()\n",
    "\n",
    "# 1) How many beers are in the JSON object?\n",
    "print(f\"Number of beers {len(beers)} \\n\")\n",
    "\n",
    "# 2) Print the response with  the pretty printer\n",
    "pprint.pprint(beers)\n",
    "\n",
    "# 3) Print the names of the beers in the JSON object using lower case characters.\n",
    "print('Beer names:', [b['name'].lower() for b in beers], ' \\n')\n",
    "pprint.pprint(beers)\n",
    "\n",
    "# 4) Select the beer Rabiator from the JSON object. What is the suggested food pairing?\n",
    "print(f'{beers[13][\"name\"]} is the 14th last entry, i.e. index 13.  \\n')\n",
    "ra = beers[13]\n",
    "print('Suggested food pairing for the Rabiator is:', set(i for i in ra['food_pairing']), '\\n')\n",
    "\n",
    "# 5) Select the beer called Hello My Name Is Beastie from the JSON object. Which malt ingredients does the Bourbon Baby contain?\n",
    "print(f'{beers[-2][\"name\"]} is the 2nd last entry, i.e. index -2.  \\n')\n",
    "bb = beers[-2]\n",
    "print('Malt ingredients in Hello My Name Is Beastie:', set(i['name'] for i in bb['ingredients']['malt']))\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.4.3:** Save the beers as a JSON file on your machine.\n",
    "\n",
    "> _Hint:_ If still in doubt about how to do this, you might want to take a look at the [json](https://docs.python.org/3/library/json.html) module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d5feea65f34d4ab1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "import json\n",
    "\n",
    "with open('beers.json', 'w') as f:\n",
    "    f.write(json.dumps(beers))\n",
    "\n",
    "with open(\"beers.json\", \"r\") as f:\n",
    "    beers = json.loads(f.read())\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The API for Statistics Denmark \n",
    "\n",
    "Statistics Denmark (DST) provide an API access to their aggregate data. For developers, they supply a [console](https://api.statbank.dk/console) for testing. In this exercise, we will code up a simple script which can collect data from the DST API. \n",
    "\n",
    "> **Ex 5.4.4:** Use the API console to construct a GET request which retrieves the table FOLK1A split by quarter. The return should be in JSON format. We want all available dates.\n",
    ">\n",
    ">Then write a function `construct_link()` which takes as inputs: a table ID (e.g. `'FOLK1A'`) and a list of strings like `['var1=*', 'var2=somevalue']`. The function should return the proper URL for getting a dataset with the specified variables (e.g. in this case all levels of var1, but only where var2=somevalue).\n",
    "\n",
    "> _Hint:_ The time variable is called 'tid'. To select all available values, set the value-id to '*'. Spend a little time with the console to get a sense of how the URLs are constructed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d174a99ed0af2f0e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "# This is the manually constructed link\n",
    "'https://api.statbank.dk/v1/data/FOLK1A/JSONSTAT?lang=en&Tid=*'\n",
    "\n",
    "# This function will do it for you\n",
    "def construct_link(table_id, variables):\n",
    "    base = f'https://api.statbank.dk/v1/data/{table_id}/JSONSTAT?lang=en'\n",
    "    \n",
    "    for var in variables:\n",
    "        base += f'&{var}'\n",
    "\n",
    "    return base \n",
    "\n",
    "construct_link('FOLK1A', ['Tid=*'])\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When executing the request in the console, you should get a json file as output. Next, lets write some code to load these json files directly into python. \n",
    "\n",
    "\n",
    "> **Ex. 5.4.5:** Use the `requests` module and `construct_link()` to request home value data from the \"EJDFOE1\" table. Get all available years (variable \"Tid\"), but only for Frederiksberg (BOPKOM=147), apartments (EJENTYP=B), market values (VAERDI=100) and average values in DKK units (ENHED=120). Unpack the json payload and store the result. Wrap the whole thing in a function which takes an url as input and returns the corresponding output.\n",
    "\n",
    "> _Hint:_ The `requests.response` object has a `.json()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-33bba55a2e8d0fc5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "# Frederiksberg\n",
    "url = construct_link('EJDFOE1', ['Tid=*', 'BOPKOM=147', 'EJENTYP=B', 'VAERDI=100', 'ENHED=120'])\n",
    "\n",
    "def create_data(url):\n",
    "    response = requests.get(url).json()\n",
    "    values = response['dataset']['value']\n",
    "    years = response['dataset']['dimension']['Tid']['category']['index']\n",
    "    years = list(map(int, years))\n",
    "    data = dict(zip(years, values))\n",
    "    return data\n",
    "\n",
    "data = create_data(url)\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.4.6:** Extract the values for Frederiksberg in each year. Store the results as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-792e1879090acb1f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "frederiksberg = list(data.values())\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.4.7:** Repeat 5.4.5 and 5.4.6 but this time only for Copenhagen (BOPKOM=101). Store the numbers in a new list and use the `plot_values` (supplied below) function to plot the (log of) the data. Interpret the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just run this once, do not change it.\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "\n",
    "def plot_values(val1, val2):\n",
    "    val1 = val1\n",
    "    val2 = val2\n",
    "    \"\"\" Plots lineplot of the number of births split by gender.\n",
    "    \n",
    "    Args: \n",
    "        boys: a list of boy births by year\n",
    "        girls: a list of girl births by year\n",
    "    \"\"\"\n",
    "    if not len(val1) == len(val2):\n",
    "        raise ValueError('There must be the same number of observations for boys and girls')\n",
    "    \n",
    "    labels = [f'{year}' for year in range(2004,2021)]\n",
    "    \n",
    "    plt.grid(color='black', linestyle='-', linewidth=0.1, axis='x')\n",
    "    \n",
    "    plt.plot(range(len(val1)), val1, color = 'blue', label = 'Frederiksberg')\n",
    "    plt.plot(range(len(val1)), val2, color = 'red', label = 'Copenhagen')\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xticks([i for i in range(len(val1)) if i%2 == 0],\n",
    "               [l for i,l in zip(range(len(val1)),labels) if i%2 == 0],\n",
    "               rotation = 'horizontal')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Market Value of Apartments')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0ea2577312192992",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "url = construct_link('EJDFOE1', ['Tid=*', 'BOPKOM=101', 'EJENTYP=B', 'VAERDI=100', 'ENHED=120'])\n",
    "\n",
    "data = create_data(url)\n",
    "odsherred = list(data.values())\n",
    "\n",
    "plot_values(frederiksberg, odsherred)\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Bonus Exercises $-$ Traffic Data in Copenhagen\n",
    "\n",
    "This set of bonus exercises allows you to apply the tools that you have learned in session 2 to 5. Note that they are not mandatory $-$ rather, our hopes are the exercises will provide you with an additional opportunity to (i) strenghten your data structuring skills and (ii) see how you have already learned a series of skills that work nicely together. \n",
    "\n",
    "In this set of bonus exercises, you will be working with traffic data from Copenhagen Municipality.\n",
    "\n",
    "The municipality have made the data openly available through the [opendata.dk](http://www.opendata.dk/) platform. We will use the data from traffic counters to construct a dataset of hourly traffic. We will use this data to get basic insights on the development in traffic over time and relate it to weather. The gist here is to practice a very important skill in Data Science: being able to quickly fetch data from the web and structure it so that you can work with it. Scraping usually gets a bit more advanced than what we will do today, but the following exercises should give you a taste for how it works. The bulk of these exercise, however, revolve around using the Pandas library to structure and analyze data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5.5.A: Getting some data to work with\n",
    "\n",
    "Hence follows a simple scraping exercise where you (1) collect urls for datasets in the webpage listing data on traffic counters and (2) use these urls to load the data into one dataframe.\n",
    "\n",
    "> **Ex. 5.5.1:** *(Bonus)* Using the requests module, extract the html markup of the webpage data.kk.dk/dataset/faste-trafiktaellinger and store it as a string in a new variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-66d30cd4ddd300f6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "from requests import get \n",
    "\n",
    "url = 'https://www.opendata.dk/city-of-copenhagen/faste-trafiktaellinger'\n",
    "resp = get(url)\n",
    "\n",
    "html = resp.text\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.5.2:** *(Bonus)* Using the re module, extract a list of all the urls in the html string and store them in a new variable.\n",
    "\n",
    "> _Hint:_ Try using the re.findall method. You may want to Google around to figure out how to do this. Searching for something along the lines of \"extract all links in html regex python\" and hitting the first StackOverflow link will probably get you farther than reading elaborate documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b6352a75b3acd612",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "### BEGIN SOLUTION\n",
    "rehits = re.findall(r'href=[\\'\"]?([^\\'\" >]+)', html)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.5.3:** *(Bonus)* Create a new variable `datalinks` that only contains the links that point to downloadable traffic data sheets. Some links may be preset more than once on the page. To get the unique links use the `set()` function on `datalinks`.\n",
    "\n",
    "> _Hint:_ You want to filter the results from above. For example to only include urls with the term 'download' in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-474f6f4274f1f6be",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "datalinks = set([url for url in rehits if 'download' in url])\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.5.4:** *(Bonus)* Using pd.read_excel method, load the datasets into a list. Your resulting variable should hold a list of Pandas dataframes.\n",
    "\n",
    "> _Hint:_ you may want to set the `skiprows` keyword argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-da5cce0bcf1ee076",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "traffic_raw = [pd.read_excel(url, skiprows = 10) for url in datalinks]\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.5.5:** *(Bonus)* Merge the list of dataframes into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4526c1736e295f0a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "final_data = pd.concat(traffic_raw)\n",
    "final_data\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5.5.B Structuring your data\n",
    "\n",
    "If you successfully completed the previous part, you should now have a dataframe with about 183.397 rows (if your number of rows is close but not the same, worry not—it matters little in the following). Well done! But the data is still in no shape for analysis, so we must clean it up a little.\n",
    "\n",
    "161.236 rows (and 30 columns) is a lot of data. ~3.3 MB by my back-of-the-envelope calculations, so not \"Big Data\", but still enough to make your CPU heat up if you don't use it carefully. Pandas is built to handle fairly large dataframes and has advanced functionality to perform very fast operations even when the size of your data grows huge. So instead of working with basic Python we recommend working pandas built-in procedures as they are constructed to be fast on dataframes.\n",
    "\n",
    "Nerd fact: the reason pandas is much faster than pure Python is that dataframes access a lower level programming languages (namely C, C++) which are multiple times faster than Python. The reason it is faster is that it has a higher level of explicitness and thus is more difficult to learn and navigate.\n",
    "\n",
    "> **Ex. 5.5.6:** *(Bonus)* Reset the row indices of your dataframe so the first index is 0 and the last is whatever the number of rows your dataframe has. Also drop the column named 'index' and the one named `Spor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bba8d42c26534466",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "final_data = final_data\\\n",
    "        .reset_index()\\\n",
    "        .drop(['index', 'Spor'], axis = 1)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.5.7:** *(Bonus)* Rename variables from Danish to English using the dictionary below.\n",
    "\n",
    "```python \n",
    "dk_to_uk = {\n",
    "    'Vejnavn':'road_name',\n",
    "    '(UTM32)':'UTM32_north',\n",
    "    '(UTM32).1':'UTM32_east',\n",
    "    'Dato':'date',\n",
    "    'Vej-Id':'road_id'\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-70b2cbb8cf1b167a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "dk_to_uk = {\n",
    "    'Vejnavn':'road_name',\n",
    "    '(UTM32)':'UTM32_north',\n",
    "    '(UTM32).1':'UTM32_east',\n",
    "    'Dato':'date',\n",
    "    'Vej-Id':'road_id'\n",
    "}\n",
    "\n",
    "\n",
    "final_data = final_data.rename(columns = dk_to_uk)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python is quite efficient. For example, when you create a new dataframe by manipulating an old one, Python notices that—apart from some minor changes—these two objects are almost the same. Since memory is a precious resource, Python will represent the values in the new dataframe as references to the variables in the old dataset. This is great for performance, but if you for whatever reason change some of the values in your old dataframe, values in the new one will also change—and we don't want that! Luckily, we can break this dependency.\n",
    "\n",
    "> **Ex. 5.5.8:** *(Bonus)* Break the dependencies of the dataframe that resulted from Ex. 5.2.7 using the `.copy` method. Delete all other dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3bc14837561aaf91",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "new_data = final_data.copy()\n",
    "\n",
    "del final_data\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have structured appropriately, something that you will want to do again and again is selecting subsets of the data. Specifically, it means that you select specific rows in the dataset based on some column values.\n",
    "\n",
    ">**Ex. 5.5.9:** *(Bonus)* Create a new column in the dataframe called total that is True when the last letter of road_id is T and otherwise False.\n",
    "\n",
    "> _Hint:_ you will need the `pd.Series.str` attribute for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d71c49b56465c6de",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "new_data['total'] = (new_data.road_id.str[-1] == 'T')\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.5.10:** *(Bonus)* Select rows where total is True. Delete all the remaining observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-be16ff389b3ac550",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "total_data = new_data[new_data.total == True]\n",
    "\n",
    "del new_data\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.5.11:** *(Bonus)* Make two datasets based on the lists of columns below. Call the dataset with spatial columns data_geo and the other data.\n",
    "\n",
    "```python\n",
    "# Columns for `geo_data`, stored in `geo_columns`\n",
    "spatial_columns = ['road_name', 'UTM32_north', 'UTM32_east']\n",
    "\n",
    "# Columns for `data`, stored in `select_columns`\n",
    "hours = ['kl.{}-{}'.format(str(h).zfill(2), str(h+1).zfill(2)) for h in range(24)]\n",
    "select_columns = ['road_name', 'date'] + hours\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-044260f4552d575c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "# Columns for `geo_data`, stored in `geo_columns`\n",
    "spatial_columns = ['road_name', 'UTM32_north', 'UTM32_east']\n",
    "\n",
    "# Columns for `data`, stored in `select_columns`\n",
    "hours = [f'kl.{str(h).zfill(2)}-{str(h+1).zfill(2)}' for h in range(24)]\n",
    "select_columns = ['road_name', 'date'] + hours\n",
    "\n",
    "# selections\n",
    "data_geo = total_data[spatial_columns]\n",
    "data  = total_data[select_columns]\n",
    "\n",
    "del total_data\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.5.12:** *(Bonus)* Drop the duplicate rows in data_geo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-34fc8fa941234a18",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "data_geo.drop_duplicates(inplace = True)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Formatting: wide and narrow format**\n",
    "\n",
    "When talking about two-dimensional data (matrices, tables or dataframes, we can call it many things), we can either say that it is in wide or long format (see explanation here, \"wide\" and \"long\" are used interchangably). In Pandas we can use the commands stack and unstack to move between these formats.\n",
    "\n",
    "The wide format has the advantage that it often requires less storage and is easier to read when printed. On the other hand the long format can be easier for modelling, because each observation has its own row. Turns out that the latter is what we most often need.\n",
    "\n",
    "> **Ex. 5.5.13:** *(Bonus)* Turn the dataset from wide to long so hourly data is now vertically stacked. Store this dataset in a dataframe called data. Name the column with hourly information hour_period. Your resulting dataframe should look something like this.\n",
    "\n",
    "> _Hint:_ pandas' melt function may be of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8814dd6f2c675c9e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "# A bonus info: This function is written using optional type declaration. This\n",
    "# is used to communicate what type the inputs should be. It doesn't affect the \n",
    "# functionality in any way, but it can sometimes increase readability. \n",
    "\n",
    "def gather(df: pd.DataFrame, key: str, value: str, cols: list) -> pd.DataFrame:\n",
    "    \"\"\" This helper makes pythons melt() behave like R's gather()\n",
    "    \"\"\"\n",
    "    id_vars = [ col for col in df.columns if col not in cols ]\n",
    "    id_values = cols\n",
    "    var_name = key\n",
    "    value_name = value\n",
    "    return pd.melt( df, id_vars, id_values, var_name, value_name )\n",
    "\n",
    "data_long = gather(data, 'hour_period', 'value', hours)\n",
    "\n",
    "# # using .stack() \n",
    "# # dropna = False in stack() is necessary for the dataframe to be exactly equal to the one above\n",
    "# data_long = data.set_index([\"road_name\", \"date\"])\\\n",
    "#                 .stack(dropna=False)\\\n",
    "#                 .reset_index()\\\n",
    "#                 .rename(columns = {\"level_2\": \"hour_period\", 0: \"value\"})\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorical data**\n",
    "\n",
    "Categorical data can contain Python objects, usually strings. These are smart if you have variables with string observations that are long and often repeated, e.g. with road names.\n",
    "\n",
    "> **Ex. 5.5.14:** *(Bonus)* Use the `.astype` method to convert the type of the road_name column to categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d94e7ec1af8fce59",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "data_long.road_name = data_long.road_name.astype('category')\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5.5.C:  Structure temporal data\n",
    "\n",
    "Pandas has native support for working with temporal data. This is handy as much 'big data' often has time stamps which we can make Pandas aware of. Once we have encoded temporal data it can be used to extract information such as the hour, second etc.\n",
    "\n",
    "> **Ex. 5.5.15:** *(Bonus)* Create a new column called hour which contains the hour-of-day for each row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-54d6bbbab97b9bae",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "data_long['hour'] = pd.to_datetime(data_long['hour_period'].str[3:5], format = '%H')\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 5.5.16:** *(Bonus)* Create a new column called time, that contains the time of the row in datetime format. Delete the old temporal columns (hour, hour_period, date) to save memory.\n",
    "\n",
    "> _Hint:_ try making an intermediary series of strings that has all temporal information for the row; then use pandas to_datetime function where you can specify the format of the date string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e8d6d81a5e31cd12",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "data_long['timestamp'] = pd.to_datetime(data_long.date + ' ' + data_long.hour\\\n",
    "                                                                        .dt\\\n",
    "                                                                        .hour\\\n",
    "                                                                        .astype(str)\\\n",
    "                                                                        .apply(lambda x: x.zfill(2)),\n",
    "                                        format = '%d.%m.%Y %H')\n",
    "\n",
    "data_long = data_long.drop(['hour', 'hour_period', 'date'], axis = 1)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.5.17:** *(Bonus)* Using your time column make a new column called weekday which stores the weekday (in values between 0 and 6) of the corresponding datetime.\n",
    "\n",
    "> _Hint:_ try using the dt method for the series called time; dt has some relevant methods itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-eeabee7e17bbc24a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "data_long['weekday'] = data_long.timestamp.dt.weekday\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5.5.D: Statistical descriptions of traffic data\n",
    "\n",
    "> **Ex. 5.5.18:** *(Bonus)* Print the \"descriptive statistics\" of the traffic column. Also show a kernel density estimate of the values.\n",
    "\n",
    "> _Hint:_ Use the describe method of pandas dataframes for the first task. Use seaborn for the second. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5543e5d8d126bc40",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "print(data_long.value.describe())\n",
    "\n",
    "sb.kdeplot(data_long.value.dropna())\n",
    "plt.show()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.5.19:** *(Bonus)* Which road has the most average traffic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ed14c5a9519b1a33",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "data_long.groupby('road_name')\\\n",
    "         .value.agg('mean')\\\n",
    "         .reset_index()\\\n",
    "         .sort_values('value', ascending = False)\\\n",
    "         .head(10)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.5.20:** *(Bonus)* Compute annual, average road traffic during day hours (9-17). Which station had the least traffic in 2013? Which station has seen highest growth in traffic from 2013 to 2014?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-279831757079d3e0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "# create year variable\n",
    "data_long['year'] = data_long.timestamp.dt.year\n",
    "data_long['hour'] = data_long.timestamp.dt.hour\n",
    "\n",
    "# subset relevant years and times \n",
    "data_delta = data_long.query(\"year == 2013 | year == 2014\")\\\n",
    "                      .query(\"hour >= 9 & hour <= 17\")\n",
    "\n",
    "# convert to wide format\n",
    "data_delta = data_delta[['road_name', 'value', 'year']].groupby(['road_name', 'year'])\\\n",
    "                                                       .agg('mean')\\\n",
    "                                                       .reset_index()\\\n",
    "\n",
    "# create year variable\n",
    "data_long['year'] = data_long.timestamp.dt.year\n",
    "data_long['hour'] = data_long.timestamp.dt.hour\n",
    "\n",
    "# subset relevant years and times \n",
    "data_delta = data_long.query(\"year == 2013 | year == 2014\")\\\n",
    "                      .query(\"hour >= 9 & hour <= 17\")\n",
    "\n",
    "# convert to wide format\n",
    "data_delta = data_delta[['road_name', 'value', 'year']].groupby(['road_name', 'year'])\\\n",
    "                                                       .agg('mean')\\\n",
    "                                                       .reset_index()\\\n",
    "                                                       .pivot('road_name', 'year', 'value')\\\n",
    "                                                       .reset_index()\n",
    "# remove redundant column name after pivoting \n",
    "data_delta.columns.name = None\n",
    "\n",
    "# calculate relative size\n",
    "data_delta['growth'] = (data_delta[2014]/data_delta[2013] - 1)*100\n",
    "\n",
    "# show\n",
    "data_delta.sort_values('growth', ascending = False)\n",
    "### END SOLUTION"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
